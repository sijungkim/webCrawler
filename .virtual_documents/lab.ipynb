from wsgiref import headers
from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin
import pandas as pd


class Crawler:
    def __init__(self, url):
        self.base_url = url
        self.visited_url = set()
        self.discovered_url = set()

    def start(self):
        current_url = self.base_url

        self.discovered_url = {current_url}
        while url_to_visit := self.discovered_url - self.visited_url:
            current_url = url_to_visit.pop()
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            try:
                res = requests.get(current_url, headers=headers, timeout=10)
                # 실패하고 말건 어차피 현재 url은 다신 안돌아갈꺼니깐 
                res.raise_for_status()
                self.visited_url.add(current_url)
            except requests.exceptions.RequestException as e:
                print(f"not able to fetch: {current_url}, with following reason: ")
                print(e)
            
            soup = BeautifulSoup(res.content, 'html.parser')
            a_list = soup.select('a')

            for link in a_list:
                url_link = urljoin(self.base_url, link.get('href'))
                self.discovered_url.add(url_link)


    def get_report(self, filename, mode):
        # discovered_url을 파일로 출력하기 위해 준비
        # set를 Series로 변환 가능한지 테스트 필요 (jupyter notebook)
        s = pd.Series(self.discovered_url)
        if mode == 'xlsx':
            s.to_excel(filename)
        elif mode == 'csv':
            s.to_csv(filename)
    


class Scraper:
    pass

scraper = Crawler("http://books.toscrape.com")
print(scraper)
