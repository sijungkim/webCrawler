{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf13137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /home/cjkim/.local/lib/python3.12/site-packages (1.45.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /home/cjkim/.local/lib/python3.12/site-packages (from streamlit) (5.5.0)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /home/cjkim/.local/lib/python3.12/site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from streamlit) (8.2.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from streamlit) (2.3.1)\n",
      "Collecting packaging<25,>=20 (from streamlit)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /home/cjkim/.local/lib/python3.12/site-packages (from streamlit) (2.2.3)\n",
      "Collecting pillow<12,>=7.1.0 (from streamlit)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /home/cjkim/.local/lib/python3.12/site-packages (from streamlit) (6.31.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /home/cjkim/.local/lib/python3.12/site-packages (from streamlit) (20.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from streamlit) (2.32.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /home/cjkim/.local/lib/python3.12/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /home/cjkim/.local/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from streamlit) (4.14.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /home/cjkim/.local/lib/python3.12/site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /home/cjkim/.local/lib/python3.12/site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /home/cjkim/.local/lib/python3.12/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: jinja2 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /home/cjkim/.local/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/cjkim/.local/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/cjkim/.local/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/cjkim/.local/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/cjkim/anaconda3/envs/mywork/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow, packaging, blinker\n",
      "\u001b[2K  Attempting uninstall: packaging━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: packaging 25.0m \u001b[32m0/3\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling packaging-25.0:━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0[0m \u001b[32m0/3\u001b[0m [pillow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [blinker]\n",
      "\u001b[1A\u001b[2KSuccessfully installed blinker-1.9.0 packaging-24.2 pillow-11.3.0\n"
     ]
    }
   ],
   "source": [
    "from wsgiref import headers\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "# import pandas as pd\n",
    "from os import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d91a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.resources import read_binary\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, url):\n",
    "        self.base_url = url\n",
    "        self.visited_url = set()\n",
    "        self.discovered_url = set()\n",
    "        self.unavailable_url = set()\n",
    "\n",
    "    def crawl(self):\n",
    "        current_url = self.base_url\n",
    "\n",
    "        self.discovered_url = {current_url}\n",
    "        while url_to_visit := self.discovered_url - self.visited_url - self.unavailable_url:\n",
    "            current_url = url_to_visit.pop()\n",
    "            # print(current_url)\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "            }\n",
    "            try:\n",
    "                res = requests.get(current_url, headers=headers, timeout=10)\n",
    "                # 실패하고 말건 어차피 현재 url은 다신 안돌아갈꺼니깐 \n",
    "                res.raise_for_status()\n",
    "                self.visited_url.add(current_url)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                # print(f\"not able to fetch: {current_url}, with following reason: \")\n",
    "                self.unavailable_url.add(current_url)\n",
    "                # print(e)\n",
    "            \n",
    "            soup = BeautifulSoup(res.content, 'html.parser')\n",
    "            a_list = soup.select('a')\n",
    "\n",
    "            for link in a_list:\n",
    "                url_link = urljoin(self.base_url, link.get('href'))\n",
    "                self.discovered_url.add(url_link)\n",
    "            \n",
    "            self.dump_result()\n",
    "    \n",
    "    def dump_result(self):\n",
    "        url_list = {\n",
    "            \"discovered\": self.discovered_url, \n",
    "            \"visited\": self.visited_url, \n",
    "            \"unavailable\": self.unavailable_url,\n",
    "            }\n",
    "        for key, val in url_list.items():\n",
    "            # 나중에 PATH 선택하는 것도 추가\n",
    "            with open(f'{key}.txt', 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(val))     \n",
    "    \n",
    "    def import_last_result(self, discovered, visited):\n",
    "        try:            \n",
    "            with open(file=discovered, mode='r', encoding='utf-8') as f:\n",
    "                self.discovered_url = set(f.read().split('\\n'))\n",
    "            with open(file=visited, mode='r', encoding='utf-8') as f:\n",
    "                self.visited_url = set(f.read().split('\\n'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "            \n",
    "# with open(file='discovered.txt', mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#     imported_text = f\n",
    "#     discovered = {f.read().split('\\n')}\n",
    "#     print(discovered)\n",
    "\n",
    "    # def get_report(self, filename, mode):\n",
    "    # def export(self):\n",
    "        # discovered_url을 파일로 출력하기 위해 준비\n",
    "        # set를 Series로 변환 가능한지 테스트 필요 (jupyter notebook)\n",
    "        # s = pd.Series(list(self.discovered_url))\n",
    "        # print(s)\n",
    "        # if mode == 'xlsx':\n",
    "        #     s.to_excel(filename)\n",
    "        # elif mode == 'csv':\n",
    "        #     s.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bb393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2c9836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Crawler object at 0x70e8ddff08f0>\n"
     ]
    }
   ],
   "source": [
    "scraper = Crawler(\"http://books.toscrape.com\")\n",
    "print(scraper)\n",
    "scraper.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ad0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.get_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2928360",
   "metadata": {},
   "outputs": [],
   "source": [
    "discovered_url = {\"apple\", \"banana\", \"cherry\", \"kiwi\"}\n",
    "visited_url = {\"apple\", \"banana\", \"cherry\", \"kiwi\"}\n",
    "unavailable_url = {\"apple\", \"banana\", \"cherry\", \"kiwi\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2746bbda",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# discovered = set()\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file=\u001b[33m'\u001b[39m\u001b[33mdiscovered.txt\u001b[39m\u001b[33m'\u001b[39m, mode=\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# content = f.read()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     discovered = {f.read().split(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)}\n\u001b[32m     44\u001b[39m     discovered.add(\u001b[33m'\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(discovered)    \n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "\n",
    "# from dis import disco\n",
    "# from shlex import join\n",
    "\n",
    "\n",
    "# discovered = '\\n'.join(discovered_url)\n",
    "# visited = '\\n'.join(visited_url)\n",
    "# unavailable  = '\\n'.join(unavailable_url)\n",
    "\n",
    "# url_list = {\"discovered\": discovered_url, \"visited\": visited_url, \"unavailable\": unavailable_url}\n",
    "\n",
    "# # print('\\n'.join(url_list[\"discovered\"]))\n",
    "\n",
    "# # url_list = [discovered, visited, unavailable]\n",
    "\n",
    "# # print(discovered_url)\n",
    "\n",
    "# # url_list_name = (lambda x: x+'.txt')(url_list)\n",
    "# # print(url_list_name)\n",
    "# # url_list = {\"discovered\": discovered_url, \"visited\": visited_url, \"unavailable\": unavailable_url}\n",
    "\n",
    "# for key, val in url_list.items():\n",
    "#     with open(f\"{key}.txt\", 'w', encoding='utf-8') as f:\n",
    "#         f.write('\\n'.join(val))\n",
    "\n",
    "# # print(url_list[0])\n",
    "\n",
    "# for i in url_list:\n",
    "#     print(i.)\n",
    "#     # with open(f\"{i}.txt\", 'w', encoding='utf-8') as f:\n",
    "#     #     f.write(i)\n",
    "from dis import disco\n",
    "from os import read\n",
    "\n",
    "\n",
    "discovered_url = {\"apple\", \"banana\", \"cherry\", \"kiwi\"}\n",
    "visited_url = {\"apple\", \"banana\", \"cherry\", \"kiwi\"}\n",
    "unavailable_url = {\"apple\", \"banana\", \"cherry\", \"kiwi\"}\n",
    "\n",
    "\n",
    "# discovered = set()\n",
    "with open(file='discovered.txt', mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    # content = f.read()\n",
    "    discovered = {f.read().split('\\n')}\n",
    "    discovered.add('last')\n",
    "print(discovered)    \n",
    "    # for i in content.split('\\n'):\n",
    "    #     discovered.add(i)\n",
    "# print(f)\n",
    "    # imported_text = f\n",
    "    # discovered = {', '.join((f.read().split('\\n'))) }\n",
    "    # print(discovered)\n",
    "\n",
    "\n",
    "\n",
    "# print(imported_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mywork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
